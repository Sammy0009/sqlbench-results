AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *(7) Sort [custdist#542L DESC NULLS LAST, c_count#547L DESC NULLS LAST], true, 0
   +- AQEShuffleRead coalesced
      +- ShuffleQueryStage 3
         +- Exchange rangepartitioning(custdist#542L DESC NULLS LAST, c_count#547L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#4279]
            +- *(6) HashAggregate(keys=[c_count#547L], functions=[count(1)], output=[c_count#547L, custdist#542L])
               +- AQEShuffleRead coalesced
                  +- ShuffleQueryStage 2
                     +- Exchange hashpartitioning(c_count#547L, 200), ENSURE_REQUIREMENTS, [id=#4233]
                        +- *(5) HashAggregate(keys=[c_count#547L], functions=[partial_count(1)], output=[c_count#547L, count#551L])
                           +- *(5) HashAggregate(keys=[c_custkey#106L], functions=[count(o_orderkey#56L)], output=[c_count#547L])
                              +- *(5) HashAggregate(keys=[c_custkey#106L], functions=[partial_count(o_orderkey#56L)], output=[c_custkey#106L, count#553L])
                                 +- *(5) Project [c_custkey#106L, o_orderkey#56L]
                                    +- *(5) SortMergeJoin [c_custkey#106L], [o_custkey#57L], LeftOuter
                                       :- *(3) Sort [c_custkey#106L ASC NULLS FIRST], false, 0
                                       :  +- AQEShuffleRead coalesced
                                       :     +- ShuffleQueryStage 0
                                       :        +- Exchange hashpartitioning(c_custkey#106L, 200), ENSURE_REQUIREMENTS, [id=#4079]
                                       :           +- *(1) ColumnarToRow
                                       :              +- FileScan parquet [c_custkey#106L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/customer.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c_custkey:bigint>
                                       +- *(4) Sort [o_custkey#57L ASC NULLS FIRST], false, 0
                                          +- AQEShuffleRead coalesced
                                             +- ShuffleQueryStage 1
                                                +- Exchange hashpartitioning(o_custkey#57L, 200), ENSURE_REQUIREMENTS, [id=#4096]
                                                   +- *(2) Project [o_orderkey#56L, o_custkey#57L]
                                                      +- *(2) Filter ((isnotnull(o_comment#64) AND NOT o_comment#64 LIKE %unusual%deposits%) AND isnotnull(o_custkey#57L))
                                                         +- *(2) ColumnarToRow
                                                            +- FileScan parquet [o_orderkey#56L,o_custkey#57L,o_comment#64] Batched: true, DataFilters: [isnotnull(o_comment#64), NOT o_comment#64 LIKE %unusual%deposits%, isnotnull(o_custkey#57L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/orders.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)], ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>
+- == Initial Plan ==
   Sort [custdist#542L DESC NULLS LAST, c_count#547L DESC NULLS LAST], true, 0
   +- Exchange rangepartitioning(custdist#542L DESC NULLS LAST, c_count#547L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#4042]
      +- HashAggregate(keys=[c_count#547L], functions=[count(1)], output=[c_count#547L, custdist#542L])
         +- Exchange hashpartitioning(c_count#547L, 200), ENSURE_REQUIREMENTS, [id=#4039]
            +- HashAggregate(keys=[c_count#547L], functions=[partial_count(1)], output=[c_count#547L, count#551L])
               +- HashAggregate(keys=[c_custkey#106L], functions=[count(o_orderkey#56L)], output=[c_count#547L])
                  +- HashAggregate(keys=[c_custkey#106L], functions=[partial_count(o_orderkey#56L)], output=[c_custkey#106L, count#553L])
                     +- Project [c_custkey#106L, o_orderkey#56L]
                        +- SortMergeJoin [c_custkey#106L], [o_custkey#57L], LeftOuter
                           :- Sort [c_custkey#106L ASC NULLS FIRST], false, 0
                           :  +- Exchange hashpartitioning(c_custkey#106L, 200), ENSURE_REQUIREMENTS, [id=#4029]
                           :     +- FileScan parquet [c_custkey#106L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/customer.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c_custkey:bigint>
                           +- Sort [o_custkey#57L ASC NULLS FIRST], false, 0
                              +- Exchange hashpartitioning(o_custkey#57L, 200), ENSURE_REQUIREMENTS, [id=#4030]
                                 +- Project [o_orderkey#56L, o_custkey#57L]
                                    +- Filter ((isnotnull(o_comment#64) AND NOT o_comment#64 LIKE %unusual%deposits%) AND isnotnull(o_custkey#57L))
                                       +- FileScan parquet [o_orderkey#56L,o_custkey#57L,o_comment#64] Batched: true, DataFilters: [isnotnull(o_comment#64), NOT o_comment#64 LIKE %unusual%deposits%, isnotnull(o_custkey#57L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/orders.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)], ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>
