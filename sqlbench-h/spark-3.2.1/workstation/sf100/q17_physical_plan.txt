AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *(7) HashAggregate(keys=[], functions=[sum(l_extendedprice#79)], output=[avg_yearly#712])
   +- ShuffleQueryStage 4
      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#5870]
         +- *(6) HashAggregate(keys=[], functions=[partial_sum(l_extendedprice#79)], output=[sum#736, isEmpty#737])
            +- *(6) Project [l_extendedprice#79]
               +- *(6) SortMergeJoin [p_partkey#14L], [l_partkey#717L], Inner, (cast(l_quantity#78 as decimal(17,7)) < (0.2 * avg(l_quantity))#715)
                  :- *(4) Sort [p_partkey#14L ASC NULLS FIRST], false, 0
                  :  +- *(4) Project [l_quantity#78, l_extendedprice#79, p_partkey#14L]
                  :     +- *(4) BroadcastHashJoin [l_partkey#75L], [p_partkey#14L], Inner, BuildRight, false
                  :        :- ShuffleQueryStage 0
                  :        :  +- Exchange hashpartitioning(l_partkey#75L, 200), ENSURE_REQUIREMENTS, [id=#5598]
                  :        :     +- *(1) Filter (isnotnull(l_partkey#75L) AND isnotnull(l_quantity#78))
                  :        :        +- *(1) ColumnarToRow
                  :        :           +- FileScan parquet [l_partkey#75L,l_quantity#78,l_extendedprice#79] Batched: true, DataFilters: [isnotnull(l_partkey#75L), isnotnull(l_quantity#78)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/lineitem.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(l_partkey), IsNotNull(l_quantity)], ReadSchema: struct<l_partkey:bigint,l_quantity:decimal(11,2),l_extendedprice:decimal(11,2)>
                  :        +- BroadcastQueryStage 3
                  :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#5719]
                  :              +- AQEShuffleRead local
                  :                 +- ShuffleQueryStage 1
                  :                    +- Exchange hashpartitioning(p_partkey#14L, 200), ENSURE_REQUIREMENTS, [id=#5617]
                  :                       +- *(2) Project [p_partkey#14L]
                  :                          +- *(2) Filter ((((isnotnull(p_brand#17) AND isnotnull(p_container#20)) AND (p_brand#17 = Brand#32)) AND (p_container#20 = SM JAR)) AND isnotnull(p_partkey#14L))
                  :                             +- *(2) ColumnarToRow
                  :                                +- FileScan parquet [p_partkey#14L,p_brand#17,p_container#20] Batched: true, DataFilters: [isnotnull(p_brand#17), isnotnull(p_container#20), (p_brand#17 = Brand#32), (p_container#20 = SM ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/part.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(p_brand), IsNotNull(p_container), EqualTo(p_brand,Brand#32), EqualTo(p_container,SM JA..., ReadSchema: struct<p_partkey:bigint,p_brand:string,p_container:string>
                  +- *(5) Sort [l_partkey#717L ASC NULLS FIRST], false, 0
                     +- *(5) Filter isnotnull((0.2 * avg(l_quantity))#715)
                        +- *(5) HashAggregate(keys=[l_partkey#717L], functions=[avg(UnscaledValue(l_quantity#720))], output=[(0.2 * avg(l_quantity))#715, l_partkey#717L])
                           +- ShuffleQueryStage 2
                              +- Exchange hashpartitioning(l_partkey#717L, 200), ENSURE_REQUIREMENTS, [id=#5640]
                                 +- *(3) HashAggregate(keys=[l_partkey#717L], functions=[partial_avg(UnscaledValue(l_quantity#720))], output=[l_partkey#717L, sum#740, count#741L])
                                    +- *(3) Filter isnotnull(l_partkey#717L)
                                       +- *(3) ColumnarToRow
                                          +- FileScan parquet [l_partkey#717L,l_quantity#720] Batched: true, DataFilters: [isnotnull(l_partkey#717L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/lineitem.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:bigint,l_quantity:decimal(11,2)>
+- == Initial Plan ==
   HashAggregate(keys=[], functions=[sum(l_extendedprice#79)], output=[avg_yearly#712])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#5549]
      +- HashAggregate(keys=[], functions=[partial_sum(l_extendedprice#79)], output=[sum#736, isEmpty#737])
         +- Project [l_extendedprice#79]
            +- SortMergeJoin [p_partkey#14L], [l_partkey#717L], Inner, (cast(l_quantity#78 as decimal(17,7)) < (0.2 * avg(l_quantity))#715)
               :- Project [l_quantity#78, l_extendedprice#79, p_partkey#14L]
               :  +- SortMergeJoin [l_partkey#75L], [p_partkey#14L], Inner
               :     :- Sort [l_partkey#75L ASC NULLS FIRST], false, 0
               :     :  +- Exchange hashpartitioning(l_partkey#75L, 200), ENSURE_REQUIREMENTS, [id=#5533]
               :     :     +- Filter (isnotnull(l_partkey#75L) AND isnotnull(l_quantity#78))
               :     :        +- FileScan parquet [l_partkey#75L,l_quantity#78,l_extendedprice#79] Batched: true, DataFilters: [isnotnull(l_partkey#75L), isnotnull(l_quantity#78)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/lineitem.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(l_partkey), IsNotNull(l_quantity)], ReadSchema: struct<l_partkey:bigint,l_quantity:decimal(11,2),l_extendedprice:decimal(11,2)>
               :     +- Sort [p_partkey#14L ASC NULLS FIRST], false, 0
               :        +- Exchange hashpartitioning(p_partkey#14L, 200), ENSURE_REQUIREMENTS, [id=#5534]
               :           +- Project [p_partkey#14L]
               :              +- Filter ((((isnotnull(p_brand#17) AND isnotnull(p_container#20)) AND (p_brand#17 = Brand#32)) AND (p_container#20 = SM JAR)) AND isnotnull(p_partkey#14L))
               :                 +- FileScan parquet [p_partkey#14L,p_brand#17,p_container#20] Batched: true, DataFilters: [isnotnull(p_brand#17), isnotnull(p_container#20), (p_brand#17 = Brand#32), (p_container#20 = SM ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/part.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(p_brand), IsNotNull(p_container), EqualTo(p_brand,Brand#32), EqualTo(p_container,SM JA..., ReadSchema: struct<p_partkey:bigint,p_brand:string,p_container:string>
               +- Sort [l_partkey#717L ASC NULLS FIRST], false, 0
                  +- Filter isnotnull((0.2 * avg(l_quantity))#715)
                     +- HashAggregate(keys=[l_partkey#717L], functions=[avg(UnscaledValue(l_quantity#720))], output=[(0.2 * avg(l_quantity))#715, l_partkey#717L])
                        +- Exchange hashpartitioning(l_partkey#717L, 200), ENSURE_REQUIREMENTS, [id=#5539]
                           +- HashAggregate(keys=[l_partkey#717L], functions=[partial_avg(UnscaledValue(l_quantity#720))], output=[l_partkey#717L, sum#740, count#741L])
                              +- Filter isnotnull(l_partkey#717L)
                                 +- FileScan parquet [l_partkey#717L,l_quantity#720] Batched: true, DataFilters: [isnotnull(l_partkey#717L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpch/sf100-parquet/lineitem.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(l_partkey)], ReadSchema: struct<l_partkey:bigint,l_quantity:decimal(11,2)>
