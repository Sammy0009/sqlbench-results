AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   TakeOrderedAndProject(limit=100, orderBy=[lochierarchy#19811 DESC NULLS LAST,CASE WHEN (lochierarchy#19811 = 0) THEN i_category#19820 END ASC NULLS FIRST,rank_within_parent#19812 ASC NULLS FIRST], output=[total_sum#19810,i_category#19820,i_class#19821,lochierarchy#19811,rank_within_parent#19812])
   +- *(6) Project [total_sum#19810, i_category#19820, i_class#19821, lochierarchy#19811, rank_within_parent#19812]
      +- Window [rank(_w3#19836) windowspecdefinition(_w1#19834, _w2#19835, _w3#19836 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#19812], [_w1#19834, _w2#19835], [_w3#19836 DESC NULLS LAST]
         +- *(5) Sort [_w1#19834 ASC NULLS FIRST, _w2#19835 ASC NULLS FIRST, _w3#19836 DESC NULLS LAST], false, 0
            +- AQEShuffleRead coalesced
               +- ShuffleQueryStage 3
                  +- Exchange hashpartitioning(_w1#19834, _w2#19835, 200), ENSURE_REQUIREMENTS, [id=#166915]
                     +- *(4) HashAggregate(keys=[i_category#19820, i_class#19821, spark_grouping_id#19819L], functions=[sum(UnscaledValue(ws_net_paid#413))], output=[total_sum#19810, i_category#19820, i_class#19821, lochierarchy#19811, _w1#19834, _w2#19835, _w3#19836])
                        +- AQEShuffleRead coalesced
                           +- ShuffleQueryStage 2
                              +- Exchange hashpartitioning(i_category#19820, i_class#19821, spark_grouping_id#19819L, 200), ENSURE_REQUIREMENTS, [id=#166862]
                                 +- *(3) HashAggregate(keys=[i_category#19820, i_class#19821, spark_grouping_id#19819L], functions=[partial_sum(UnscaledValue(ws_net_paid#413))], output=[i_category#19820, i_class#19821, spark_grouping_id#19819L, sum#19849L])
                                    +- *(3) Expand [[ws_net_paid#413, i_category#512, i_class#510, 0], [ws_net_paid#413, i_category#512, null, 1], [ws_net_paid#413, null, null, 3]], [ws_net_paid#413, i_category#19820, i_class#19821, spark_grouping_id#19819L]
                                       +- *(3) Project [ws_net_paid#413, i_category#512, i_class#510]
                                          +- *(3) BroadcastHashJoin [ws_item_sk#387], [i_item_sk#500], Inner, BuildRight, false
                                             :- *(3) Project [ws_item_sk#387, ws_net_paid#413]
                                             :  +- *(3) BroadcastHashJoin [ws_sold_date_sk#384], [d_date_sk#598], Inner, BuildRight, false
                                             :     :- *(3) Filter (isnotnull(ws_sold_date_sk#384) AND isnotnull(ws_item_sk#387))
                                             :     :  +- *(3) ColumnarToRow
                                             :     :     +- FileScan parquet [ws_sold_date_sk#384,ws_item_sk#387,ws_net_paid#413] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#384), isnotnull(ws_item_sk#387)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/web_sales.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_net_paid:decimal(7,2)>
                                             :     +- BroadcastQueryStage 0
                                             :        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#166677]
                                             :           +- *(1) Project [d_date_sk#598]
                                             :              +- *(1) Filter (((isnotnull(d_month_seq#601) AND (d_month_seq#601 >= 1205)) AND (d_month_seq#601 <= 1216)) AND isnotnull(d_date_sk#598))
                                             :                 +- *(1) ColumnarToRow
                                             :                    +- FileScan parquet [d_date_sk#598,d_month_seq#601] Batched: true, DataFilters: [isnotnull(d_month_seq#601), (d_month_seq#601 >= 1205), (d_month_seq#601 <= 1216), isnotnull(d_da..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/date_dim.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1205), LessThanOrEqual(d_month_seq,1216),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                                             +- BroadcastQueryStage 1
                                                +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#166696]
                                                   +- *(2) Filter isnotnull(i_item_sk#500)
                                                      +- *(2) ColumnarToRow
                                                         +- FileScan parquet [i_item_sk#500,i_class#510,i_category#512] Batched: true, DataFilters: [isnotnull(i_item_sk#500)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/item.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_class:string,i_category:string>
+- == Initial Plan ==
   TakeOrderedAndProject(limit=100, orderBy=[lochierarchy#19811 DESC NULLS LAST,CASE WHEN (lochierarchy#19811 = 0) THEN i_category#19820 END ASC NULLS FIRST,rank_within_parent#19812 ASC NULLS FIRST], output=[total_sum#19810,i_category#19820,i_class#19821,lochierarchy#19811,rank_within_parent#19812])
   +- Project [total_sum#19810, i_category#19820, i_class#19821, lochierarchy#19811, rank_within_parent#19812]
      +- Window [rank(_w3#19836) windowspecdefinition(_w1#19834, _w2#19835, _w3#19836 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#19812], [_w1#19834, _w2#19835], [_w3#19836 DESC NULLS LAST]
         +- Sort [_w1#19834 ASC NULLS FIRST, _w2#19835 ASC NULLS FIRST, _w3#19836 DESC NULLS LAST], false, 0
            +- Exchange hashpartitioning(_w1#19834, _w2#19835, 200), ENSURE_REQUIREMENTS, [id=#166632]
               +- HashAggregate(keys=[i_category#19820, i_class#19821, spark_grouping_id#19819L], functions=[sum(UnscaledValue(ws_net_paid#413))], output=[total_sum#19810, i_category#19820, i_class#19821, lochierarchy#19811, _w1#19834, _w2#19835, _w3#19836])
                  +- Exchange hashpartitioning(i_category#19820, i_class#19821, spark_grouping_id#19819L, 200), ENSURE_REQUIREMENTS, [id=#166629]
                     +- HashAggregate(keys=[i_category#19820, i_class#19821, spark_grouping_id#19819L], functions=[partial_sum(UnscaledValue(ws_net_paid#413))], output=[i_category#19820, i_class#19821, spark_grouping_id#19819L, sum#19849L])
                        +- Expand [[ws_net_paid#413, i_category#512, i_class#510, 0], [ws_net_paid#413, i_category#512, null, 1], [ws_net_paid#413, null, null, 3]], [ws_net_paid#413, i_category#19820, i_class#19821, spark_grouping_id#19819L]
                           +- Project [ws_net_paid#413, i_category#512, i_class#510]
                              +- BroadcastHashJoin [ws_item_sk#387], [i_item_sk#500], Inner, BuildRight, false
                                 :- Project [ws_item_sk#387, ws_net_paid#413]
                                 :  +- BroadcastHashJoin [ws_sold_date_sk#384], [d_date_sk#598], Inner, BuildRight, false
                                 :     :- Filter (isnotnull(ws_sold_date_sk#384) AND isnotnull(ws_item_sk#387))
                                 :     :  +- FileScan parquet [ws_sold_date_sk#384,ws_item_sk#387,ws_net_paid#413] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#384), isnotnull(ws_item_sk#387)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/web_sales.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_net_paid:decimal(7,2)>
                                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#166619]
                                 :        +- Project [d_date_sk#598]
                                 :           +- Filter (((isnotnull(d_month_seq#601) AND (d_month_seq#601 >= 1205)) AND (d_month_seq#601 <= 1216)) AND isnotnull(d_date_sk#598))
                                 :              +- FileScan parquet [d_date_sk#598,d_month_seq#601] Batched: true, DataFilters: [isnotnull(d_month_seq#601), (d_month_seq#601 >= 1205), (d_month_seq#601 <= 1216), isnotnull(d_da..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/date_dim.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1205), LessThanOrEqual(d_month_seq,1216),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#166623]
                                    +- Filter isnotnull(i_item_sk#500)
                                       +- FileScan parquet [i_item_sk#500,i_class#510,i_category#512] Batched: true, DataFilters: [isnotnull(i_item_sk#500)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/bigdata/tpcds/sf100-parquet/item.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_class:string,i_category:string>
